{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM3wxLOxM3dMwbmYb55rXu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pberlizov/pberlizov/blob/main/SLAI_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9vYDNOn2mD3",
        "outputId": "0a676d87-d123-4a4d-8507-831a901a2335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What’s the capital of Brazil?\n",
            "Routed to: meta.llama3-8b\n",
            "Response: Response from meta.llama3-8b: Processed 'What’s the capital of Brazil?'\n",
            "\n",
            "Query: Write a 500-word essay on renewable energy solutions.\n",
            "Routed to: meta.llama3-8b\n",
            "Response: Response from meta.llama3-8b: Processed 'Write a 500-word essay on renewable energy solutions.'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Bedrock model performance data (hypothetical 2025 ratings)\n",
        "# Format: {model_id: {\"size\": GB, \"energy\": kWh/inference, \"cost\": $/1K tokens,\n",
        "#\"latency\": seconds, \"accuracy\": 0-1}}\n",
        "\n",
        "# For the future, makes sense to set up a system to get these ratings\n",
        "# automatically, like a scraper? Mid-term step. Picked just three models.\n",
        "# No real sense in going much further given the basic prototype here.\n",
        "MODELS = {\n",
        "    \"meta.llama3-8b\": {\n",
        "        \"size\": 16.0,          # 8B params, ~16GB\n",
        "        \"energy\": 0.0001,      # Low energy estimate\n",
        "        \"cost\": 0.0003,        # Bedrock pricing guess\n",
        "        \"latency\": 0.5,        # Fast for small model\n",
        "        \"accuracy\": 0.85       # Decent accuracy\n",
        "    },\n",
        "    \"mistral.mixtral-8x7b\": {\n",
        "        \"size\": 90.0,          # 56B params, ~90GB\n",
        "        \"energy\": 0.0003,      # Higher energy\n",
        "        \"cost\": 0.0005,        # More expensive\n",
        "        \"latency\": 0.7,        # Slower due to size\n",
        "        \"accuracy\": 0.90       # High accuracy\n",
        "    },\n",
        "    \"amazon.titan-text\": {\n",
        "        \"size\": 30.0,          # Mid-size proprietary\n",
        "        \"energy\": 0.00015,     # Moderate energy\n",
        "        \"cost\": 0.0004,        # Mid-range cost\n",
        "        \"latency\": 0.4,        # Optimized for speed\n",
        "        \"accuracy\": 0.80       # Lower accuracy\n",
        "    }\n",
        "}\n",
        "\n",
        "# Default weights (equal for now, sum to 1.0)\n",
        "# Just picked a weighted sum, since we discussed the function being immaterial.\n",
        "WEIGHTS = {\n",
        "    \"size\": 0.2,\n",
        "    \"energy\": 0.2,\n",
        "    \"cost\": 0.2,\n",
        "    \"latency\": 0.2,\n",
        "    \"accuracy\": 0.2\n",
        "}\n",
        "\n",
        "# Classifying by length for now. Once we get ratings, will switch to subject\n",
        "# matter ratings and semantic meaning. That would require API to LLM integration\n",
        "def classify_query(query: str) -> float:\n",
        "    \"\"\"Estimate query complexity based on word count (0-1 scale).\"\"\"\n",
        "    word_count = len(query.split())\n",
        "    complexity = min(word_count / 50, 1.0)  # Normalize: 0-50 words = 0-1\n",
        "    return complexity\n",
        "\n",
        "# Normalizing the values of the ratings. Could input them that way, but this\n",
        "# strikes me as more convenient.\n",
        "def normalize_metric(value: float, min_val: float, max_val: float) -> float:\n",
        "    \"\"\"Normalize a metric to 0-1 scale.\"\"\"\n",
        "    return (value - min_val) / (max_val - min_val) if max_val > min_val else 0\n",
        "\n",
        "def score_model(model_metrics: dict, complexity: float) -> float:\n",
        "    \"\"\"Calculate a score for a model (lower is better).\"\"\"\n",
        "    # Extract raw values\n",
        "    size = model_metrics[\"size\"]\n",
        "    energy = model_metrics[\"energy\"]\n",
        "    cost = model_metrics[\"cost\"]\n",
        "    latency = model_metrics[\"latency\"]\n",
        "    accuracy = model_metrics[\"accuracy\"]\n",
        "\n",
        "    # Define ranges for normalization (based on sample data)\n",
        "    ranges = {\n",
        "        \"size\": (16.0, 90.0),       # GB\n",
        "        \"energy\": (0.0001, 0.0003), # kWh/inference\n",
        "        \"cost\": (0.0003, 0.0005),   # $/1K tokens\n",
        "        \"latency\": (0.4, 0.7),      # seconds\n",
        "        \"accuracy\": (0.80, 0.90)    # 0-1\n",
        "    }\n",
        "\n",
        "    # Normalize metrics (invert accuracy: higher = better)\n",
        "    norm_size = normalize_metric(size, *ranges[\"size\"])\n",
        "    norm_energy = normalize_metric(energy, *ranges[\"energy\"])\n",
        "    norm_cost = normalize_metric(cost, *ranges[\"cost\"])\n",
        "    norm_latency = normalize_metric(latency, *ranges[\"latency\"])\n",
        "    norm_accuracy = 1 - normalize_metric(accuracy, *ranges[\"accuracy\"])  # Invert\n",
        "\n",
        "    # Adjust weights dynamically with complexity (optional for future RL)\n",
        "    # For now, use static weights\n",
        "    score = (WEIGHTS[\"size\"] * norm_size +\n",
        "             WEIGHTS[\"energy\"] * norm_energy +\n",
        "             WEIGHTS[\"cost\"] * norm_cost +\n",
        "             WEIGHTS[\"latency\"] * norm_latency +\n",
        "             WEIGHTS[\"accuracy\"] * norm_accuracy)\n",
        "    return score\n",
        "\n",
        "# Simplest possible routing.\n",
        "def route_query(query: str) -> str:\n",
        "    \"\"\"Route query to the most efficient model.\"\"\"\n",
        "    complexity = classify_query(query)\n",
        "    scores = {model_id: score_model(metrics, complexity) for model_id, metrics in MODELS.items()}\n",
        "    best_model = min(scores, key=scores.get)  # Lowest score wins\n",
        "    return best_model\n",
        "\n",
        "# Just a placeholder for now.\n",
        "def simulate_api_call(model_id: str, query: str) -> str:\n",
        "    \"\"\"Simulate calling the model's API (replace with Bedrock API later).\"\"\"\n",
        "    return f\"Response from {model_id}: Processed '{query}'\"\n",
        "\n",
        "# Test the prototype\n",
        "if __name__ == \"__main__\":\n",
        "    queries = [\n",
        "        \"What’s the capital of Brazil?\",  # Short, simple\n",
        "        \"Write a 500-word essay on renewable energy solutions.\"  # Long, complex\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        model_id = route_query(query)\n",
        "        response = simulate_api_call(model_id, query)\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Routed to: {model_id}\")\n",
        "        print(f\"Response: {response}\\n\")"
      ]
    }
  ]
}